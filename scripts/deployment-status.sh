#!/bin/bash

echo "üöÄ Airflow-Elyra Deployment Status Summary"
echo "=========================================="
echo

echo "‚úÖ DEPLOYMENT STATUS: SUCCESSFUL"
echo "‚úÖ GitHub DAG Sync: WORKING"
echo "‚úÖ Airflow Authentication: WORKING (admin/admin)"
echo "‚úÖ SparkKubernetesOperator: WORKING"
echo "‚úÖ S3 Storage (LocalStack): WORKING"
echo "‚úÖ JupyterLab: WORKING"
echo "‚úÖ Papermill Integration: WORKING"
echo "    echo "  üîÑ Elyra: IN PROGRESS - Extension installs correctly but requires container restart to load server endpoints""
echo

echo "üåê Service Endpoints:"
echo "--------------------"
echo "  ‚Ä¢ Airflow UI: http://localhost:8080"
echo "  ‚Ä¢ JupyterLab: http://localhost:8081 (token: datascience123)"
echo "  ‚Ä¢ LocalStack S3: http://localhost:4566"
echo

echo "üìä Recent Spark Job Execution:"
echo "-----------------------------"
kubectl exec -it deployment/localstack -n localstack -- env AWS_ACCESS_KEY_ID=test AWS_SECRET_ACCESS_KEY=test aws --endpoint-url=http://localhost:4566 s3 ls s3://data-engineering-bucket/spark-output/ --recursive | head -5
echo

echo "üìö JupyterLab Extensions:"
echo "-----------------------"
kubectl exec -n jupyter deployment/jupyter-lab -- jupyter labextension list | grep -E "(elyra|papermill)" | head -5
echo

echo "üìù Available Demo Notebooks:"
echo "----------------------------"
echo "  ‚Ä¢ notebooks/papermill_demo.ipynb - Papermill integration demo"
echo "  ‚Ä¢ Can be executed from Airflow using PapermillOperator"
echo "  ‚Ä¢ Note: Elyra visual pipeline editor has server endpoint issues"
echo "  ‚Ä¢ Workaround: Use Papermill directly or create pipelines via code"
echo

echo "üîß Quick Test Commands:"
echo "----------------------"
echo "  # Test Airflow DAG execution:"
echo "  kubectl exec -n airflow airflow-scheduler-0 -- airflow dags list"
echo
echo "  # Test Spark job execution:"
echo "  kubectl exec -n airflow airflow-worker-0 -c worker -- airflow tasks test spark_operator_s3_pipeline spark_data_processing 2025-07-24"
echo
echo "  # Access JupyterLab:"
echo "  open http://localhost:8081"
echo

echo "üì¶ Component Status:"
echo "------------------"
kubectl get pods -A | grep -E "(airflow|jupyter|spark|localstack)" | awk '{print "  " $2 ": " $4}'
echo

echo "üéâ READY FOR USE!"
echo "================"
echo "‚Ä¢ All core components are operational"
echo "‚Ä¢ GitHub DAG synchronization is active"
echo "‚Ä¢ Spark jobs can execute successfully"
echo "‚Ä¢ Jupyter notebooks with Papermill can be executed from Airflow"
echo "‚Ä¢ S3 storage is available via LocalStack"
echo
echo "For full deployment, run: ./scripts/deploy-everything.sh"
