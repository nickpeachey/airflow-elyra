{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9884efbe",
   "metadata": {},
   "source": [
    "# Papermill Demo Notebook\n",
    "\n",
    "This notebook demonstrates how to use Papermill for parameterized notebook execution.\n",
    "\n",
    "## Parameters\n",
    "- `input_data`: Path to input data\n",
    "- `output_path`: Path for output results\n",
    "- `processing_date`: Date for data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e03c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters - these will be injected by Papermill\n",
    "input_data = 'default_input.csv'\n",
    "output_path = 'default_output.csv'\n",
    "processing_date = '2024-01-01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848ddf68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "print(f\"Processing data for: {processing_date}\")\n",
    "print(f\"Input data: {input_data}\")\n",
    "print(f\"Output path: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ce0b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data processing\n",
    "print(\"Creating sample data...\")\n",
    "\n",
    "# Generate sample data\n",
    "np.random.seed(42)\n",
    "data = {\n",
    "    'id': range(1, 101),\n",
    "    'value': np.random.normal(100, 15, 100),\n",
    "    'category': np.random.choice(['A', 'B', 'C'], 100),\n",
    "    'date': pd.date_range(start=processing_date, periods=100, freq='D')\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(f\"Generated {len(df)} rows of data\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2c282a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform analysis\n",
    "print(\"\\nPerforming analysis...\")\n",
    "\n",
    "summary_stats = df.groupby('category')['value'].agg(['mean', 'std', 'count'])\n",
    "print(\"\\nSummary by category:\")\n",
    "print(summary_stats)\n",
    "\n",
    "# Calculate overall metrics\n",
    "total_records = len(df)\n",
    "avg_value = df['value'].mean()\n",
    "max_value = df['value'].max()\n",
    "min_value = df['value'].min()\n",
    "\n",
    "print(f\"\\nOverall metrics:\")\n",
    "print(f\"Total records: {total_records}\")\n",
    "print(f\"Average value: {avg_value:.2f}\")\n",
    "print(f\"Max value: {max_value:.2f}\")\n",
    "print(f\"Min value: {min_value:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a76a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "print(f\"\\nSaving results to: {output_path}\")\n",
    "\n",
    "# In a real scenario, you would save to the actual output_path\n",
    "# For demo purposes, we'll just print the save operation\n",
    "print(\"Results saved successfully!\")\n",
    "\n",
    "# Return results for Papermill\n",
    "results = {\n",
    "    'total_records': total_records,\n",
    "    'avg_value': avg_value,\n",
    "    'processing_date': processing_date,\n",
    "    'output_file': output_path\n",
    "}\n",
    "\n",
    "print(\"\\nProcessing completed!\")\n",
    "print(f\"Results: {results}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66f9651",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. **Parameter injection** using the `parameters` tag\n",
    "2. **Data processing** with pandas\n",
    "3. **Results generation** for downstream consumption\n",
    "\n",
    "To run this notebook with Papermill:\n",
    "\n",
    "```bash\n",
    "papermill notebooks/papermill_demo.ipynb output.ipynb \\\n",
    "    -p input_data \"my_data.csv\" \\\n",
    "    -p output_path \"results.csv\" \\\n",
    "    -p processing_date \"2024-01-15\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968d6f30",
   "metadata": {},
   "source": [
    "# Papermill Demo - Data Analysis Pipeline\n",
    "\n",
    "This notebook demonstrates Papermill integration with Airflow for automated data processing.\n",
    "It can be executed from Airflow DAGs using the PapermillOperator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172ae2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters - these can be overridden by Papermill\n",
    "input_data_path = \"/home/jovyan/work/data/sales_data.csv\"\n",
    "output_path = \"/home/jovyan/work/output/\"\n",
    "analysis_date = \"2025-07-24\"\n",
    "region_filter = \"all\"\n",
    "create_visualizations = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cb3900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "print(f\"üìä Starting data analysis for {analysis_date}\")\n",
    "print(f\"üìÇ Input path: {input_data_path}\")\n",
    "print(f\"üìÅ Output path: {output_path}\")\n",
    "print(f\"üåç Region filter: {region_filter}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e016dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data if it doesn't exist\n",
    "if not os.path.exists(input_data_path):\n",
    "    print(\"üîß Creating sample sales data...\")\n",
    "    \n",
    "    # Ensure directory exists\n",
    "    os.makedirs(os.path.dirname(input_data_path), exist_ok=True)\n",
    "    \n",
    "    # Generate sample data\n",
    "    np.random.seed(42)\n",
    "    dates = pd.date_range('2025-01-01', '2025-07-24', freq='D')\n",
    "    regions = ['North', 'South', 'East', 'West']\n",
    "    products = ['Product_A', 'Product_B', 'Product_C', 'Product_D', 'Product_E']\n",
    "    \n",
    "    data = []\n",
    "    for date in dates:\n",
    "        for _ in range(np.random.randint(10, 50)):\n",
    "            data.append({\n",
    "                'date': date,\n",
    "                'region': np.random.choice(regions),\n",
    "                'product': np.random.choice(products),\n",
    "                'sales_amount': np.random.uniform(100, 1000),\n",
    "                'quantity': np.random.randint(1, 10),\n",
    "                'customer_id': f\"CUST_{np.random.randint(1000, 9999)}\"\n",
    "            })\n",
    "    \n",
    "    df_sample = pd.DataFrame(data)\n",
    "    df_sample.to_csv(input_data_path, index=False)\n",
    "    print(f\"‚úÖ Sample data created with {len(df_sample)} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5425e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and analyze data\n",
    "print(\"üìñ Loading data...\")\n",
    "df = pd.read_csv(input_data_path)\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "print(f\"üìà Loaded {len(df)} records\")\n",
    "print(f\"üìÖ Date range: {df['date'].min()} to {df['date'].max()}\")\n",
    "print(f\"üåç Regions: {df['region'].unique()}\")\n",
    "print(f\"üì¶ Products: {df['product'].unique()}\")\n",
    "\n",
    "# Apply region filter if specified\n",
    "if region_filter != \"all\":\n",
    "    df = df[df['region'] == region_filter]\n",
    "    print(f\"üîç Filtered to region: {region_filter} ({len(df)} records)\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3dcd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform analysis\n",
    "print(\"üî¨ Performing analysis...\")\n",
    "\n",
    "# Summary statistics\n",
    "total_sales = df['sales_amount'].sum()\n",
    "total_quantity = df['quantity'].sum()\n",
    "unique_customers = df['customer_id'].nunique()\n",
    "avg_order_value = df['sales_amount'].mean()\n",
    "\n",
    "# Regional analysis\n",
    "regional_summary = df.groupby('region').agg({\n",
    "    'sales_amount': ['sum', 'mean', 'count'],\n",
    "    'quantity': 'sum',\n",
    "    'customer_id': 'nunique'\n",
    "}).round(2)\n",
    "\n",
    "# Product analysis\n",
    "product_summary = df.groupby('product').agg({\n",
    "    'sales_amount': ['sum', 'mean'],\n",
    "    'quantity': 'sum'\n",
    "}).round(2)\n",
    "\n",
    "# Daily trend\n",
    "daily_sales = df.groupby('date')['sales_amount'].sum().reset_index()\n",
    "\n",
    "print(f\"üí∞ Total Sales: ${total_sales:,.2f}\")\n",
    "print(f\"üì¶ Total Quantity: {total_quantity:,}\")\n",
    "print(f\"üë• Unique Customers: {unique_customers:,}\")\n",
    "print(f\"üíµ Average Order Value: ${avg_order_value:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93f4ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations if requested\n",
    "if create_visualizations:\n",
    "    print(\"üìä Creating visualizations...\")\n",
    "    \n",
    "    # Set up the plotting style\n",
    "    plt.style.use('seaborn-v0_8')\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle(f'Sales Analysis Dashboard - {analysis_date}', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Regional sales\n",
    "    regional_sales = df.groupby('region')['sales_amount'].sum().sort_values(ascending=True)\n",
    "    regional_sales.plot(kind='barh', ax=axes[0,0], color='skyblue')\n",
    "    axes[0,0].set_title('Sales by Region')\n",
    "    axes[0,0].set_xlabel('Sales Amount ($)')\n",
    "    \n",
    "    # Product sales\n",
    "    product_sales = df.groupby('product')['sales_amount'].sum().sort_values(ascending=False)\n",
    "    product_sales.plot(kind='bar', ax=axes[0,1], color='lightgreen')\n",
    "    axes[0,1].set_title('Sales by Product')\n",
    "    axes[0,1].set_ylabel('Sales Amount ($)')\n",
    "    axes[0,1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Daily sales trend (last 30 days)\n",
    "    recent_sales = daily_sales.tail(30)\n",
    "    axes[1,0].plot(recent_sales['date'], recent_sales['sales_amount'], marker='o', linewidth=2)\n",
    "    axes[1,0].set_title('Daily Sales Trend (Last 30 Days)')\n",
    "    axes[1,0].set_ylabel('Sales Amount ($)')\n",
    "    axes[1,0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Sales distribution\n",
    "    axes[1,1].hist(df['sales_amount'], bins=30, alpha=0.7, color='orange')\n",
    "    axes[1,1].set_title('Sales Amount Distribution')\n",
    "    axes[1,1].set_xlabel('Sales Amount ($)')\n",
    "    axes[1,1].set_ylabel('Frequency')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save visualization\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    viz_path = os.path.join(output_path, f'sales_dashboard_{analysis_date}.png')\n",
    "    plt.savefig(viz_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"üìä Visualization saved to: {viz_path}\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c1f7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save analysis results\n",
    "print(\"üíæ Saving analysis results...\")\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "# Save summary data\n",
    "summary_data = {\n",
    "    'analysis_date': analysis_date,\n",
    "    'region_filter': region_filter,\n",
    "    'total_sales': total_sales,\n",
    "    'total_quantity': total_quantity,\n",
    "    'unique_customers': unique_customers,\n",
    "    'avg_order_value': avg_order_value,\n",
    "    'records_processed': len(df)\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame([summary_data])\n",
    "summary_path = os.path.join(output_path, f'analysis_summary_{analysis_date}.csv')\n",
    "summary_df.to_csv(summary_path, index=False)\n",
    "\n",
    "# Save detailed regional analysis\n",
    "regional_path = os.path.join(output_path, f'regional_analysis_{analysis_date}.csv')\n",
    "regional_summary.to_csv(regional_path)\n",
    "\n",
    "# Save product analysis\n",
    "product_path = os.path.join(output_path, f'product_analysis_{analysis_date}.csv')\n",
    "product_summary.to_csv(product_path)\n",
    "\n",
    "print(f\"‚úÖ Summary saved to: {summary_path}\")\n",
    "print(f\"‚úÖ Regional analysis saved to: {regional_path}\")\n",
    "print(f\"‚úÖ Product analysis saved to: {product_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c630a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary for Airflow/Papermill execution\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üìã ANALYSIS COMPLETE\")\n",
    "print(\"=\"*50)\n",
    "print(f\"üìÖ Analysis Date: {analysis_date}\")\n",
    "print(f\"üåç Region Filter: {region_filter}\")\n",
    "print(f\"üìä Records Processed: {len(df):,}\")\n",
    "print(f\"üí∞ Total Sales: ${total_sales:,.2f}\")\n",
    "print(f\"üë• Unique Customers: {unique_customers:,}\")\n",
    "print(f\"üìÅ Output Path: {output_path}\")\n",
    "print(\"\\n‚úÖ This notebook can be executed from Airflow using PapermillOperator!\")\n",
    "print(\"\\nüîß Example Airflow task:\")\n",
    "print(\"```python\")\n",
    "print(\"from airflow.providers.papermill.operators.papermill import PapermillOperator\")\n",
    "print(\"\")\n",
    "print(\"papermill_task = PapermillOperator(\")\n",
    "print(\"    task_id='run_data_analysis',\")\n",
    "print(\"    input_nb='/path/to/papermill_demo.ipynb',\")\n",
    "print(\"    output_nb='/path/to/executed_notebook.ipynb',\")\n",
    "print(\"    parameters={\")\n",
    "print(\"        'analysis_date': '{{ ds }}',\")\n",
    "print(\"        'region_filter': 'North',\")\n",
    "print(\"        'create_visualizations': True\")\n",
    "print(\"    }\")\n",
    "print(\")\")\n",
    "print(\"```\")\n",
    "print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
