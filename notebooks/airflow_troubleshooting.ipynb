{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c253092",
   "metadata": {},
   "source": [
    "# Airflow DAG & Papermill Troubleshooting Guide\n",
    "\n",
    "This notebook helps troubleshoot common issues with Airflow DAG visibility and Papermill notebook integration in a Kubernetes deployment.\n",
    "\n",
    "## Overview\n",
    "- Check Airflow configuration and pod status\n",
    "- Verify DAG directory structure and file permissions  \n",
    "- Debug DAG loading and parsing issues\n",
    "- Validate Papermill integration and notebook execution\n",
    "- Test Kubernetes pod connectivity and resource allocation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfa22eb",
   "metadata": {},
   "source": [
    "## 1. Check Airflow Configuration and Status\n",
    "\n",
    "First, let's check the status of our Airflow deployment and pods to identify any potential issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8f6d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def run_kubectl_command(command):\n",
    "    \"\"\"Execute a kubectl command and return the output\"\"\"\n",
    "    try:\n",
    "        result = subprocess.run(command, shell=True, capture_output=True, text=True)\n",
    "        if result.returncode == 0:\n",
    "            return result.stdout.strip()\n",
    "        else:\n",
    "            return f\"Error: {result.stderr.strip()}\"\n",
    "    except Exception as e:\n",
    "        return f\"Exception: {str(e)}\"\n",
    "\n",
    "# Check Airflow pod status\n",
    "print(\"=== Airflow Pod Status ===\")\n",
    "pod_status = run_kubectl_command(\"kubectl get pods -n airflow\")\n",
    "print(pod_status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a62ab65",
   "metadata": {},
   "source": [
    "## 2. Verify DAG Directory and File Structure\n",
    "\n",
    "Let's inspect the DAG directory structure and check if DAG files exist in the correct location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0505c2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get scheduler pod name\n",
    "scheduler_pod = run_kubectl_command(\"kubectl get pods -n airflow -l component=scheduler -o jsonpath='{.items[0].metadata.name}'\")\n",
    "print(f\"Scheduler pod: {scheduler_pod}\")\n",
    "\n",
    "# Check DAG directory contents\n",
    "print(\"\\n=== DAG Directory Contents ===\")\n",
    "dag_contents = run_kubectl_command(f\"kubectl exec -n airflow {scheduler_pod} -c scheduler -- ls -la /opt/airflow/dags/\")\n",
    "print(dag_contents)\n",
    "\n",
    "# Check if specific DAG files exist\n",
    "print(\"\\n=== Checking for specific DAG files ===\")\n",
    "dag_files = ['scala_spark_pipeline.py', 'papermill_example.py', 'data_processing_pipeline.py']\n",
    "for dag_file in dag_files:\n",
    "    check_result = run_kubectl_command(f\"kubectl exec -n airflow {scheduler_pod} -c scheduler -- test -f /opt/airflow/dags/{dag_file} && echo 'EXISTS' || echo 'NOT FOUND'\")\n",
    "    print(f\"{dag_file}: {check_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c799c23",
   "metadata": {},
   "source": [
    "## 3. Debug DAG Loading Issues\n",
    "\n",
    "Let's check the Airflow scheduler and webserver logs for any DAG parsing errors or import failures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ffda57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check scheduler logs for DAG parsing errors\n",
    "print(\"=== Scheduler Logs (last 50 lines) ===\")\n",
    "scheduler_logs = run_kubectl_command(f\"kubectl logs -n airflow {scheduler_pod} -c scheduler --tail=50\")\n",
    "print(scheduler_logs)\n",
    "\n",
    "# Check for specific error patterns\n",
    "print(\"\\n=== Checking for Import Errors ===\")\n",
    "import_errors = run_kubectl_command(f\"kubectl logs -n airflow {scheduler_pod} -c scheduler | grep -i 'import.*error\\\\|modulenotfound\\\\|syntax.*error' | tail -10\")\n",
    "if import_errors:\n",
    "    print(import_errors)\n",
    "else:\n",
    "    print(\"No import errors found in recent logs\")\n",
    "\n",
    "# Test DAG syntax by trying to parse one\n",
    "print(\"\\n=== Testing DAG Syntax ===\")\n",
    "dag_test = run_kubectl_command(f\"kubectl exec -n airflow {scheduler_pod} -c scheduler -- python -m py_compile /opt/airflow/dags/scala_spark_pipeline.py\")\n",
    "print(f\"DAG syntax test result: {dag_test if dag_test else 'No syntax errors'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f975f42f",
   "metadata": {},
   "source": [
    "## 4. Inspect Papermill Integration\n",
    "\n",
    "Let's verify Papermill installation and check if notebook files are accessible within the Airflow environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23678ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if Papermill is installed in Airflow scheduler\n",
    "print(\"Checking Papermill availability in Airflow scheduler...\")\n",
    "result = subprocess.run(['kubectl', 'exec', '-n', 'airflow', '-l', 'component=scheduler', '--', 'python', '-c', 'import papermill; print(papermill.__version__)'], \n",
    "                       capture_output=True, text=True)\n",
    "print(f\"Papermill in scheduler: {result.stdout.strip() if result.returncode == 0 else 'Not found or error'}\")\n",
    "if result.stderr:\n",
    "    print(f\"Error: {result.stderr}\")\n",
    "\n",
    "# Check if notebook files are accessible from Airflow\n",
    "print(\"\\nChecking notebook accessibility in Airflow scheduler...\")\n",
    "result = subprocess.run(['kubectl', 'exec', '-n', 'airflow', '-l', 'component=scheduler', '--', 'ls', '-la', '/opt/airflow/notebooks/'], \n",
    "                       capture_output=True, text=True)\n",
    "print(f\"Notebooks in scheduler: {result.stdout if result.returncode == 0 else 'Directory not found'}\")\n",
    "if result.stderr:\n",
    "    print(f\"Error: {result.stderr}\")\n",
    "\n",
    "# Check if notebooks directory exists and has content\n",
    "print(\"\\nChecking for notebooks in common locations...\")\n",
    "locations = ['/opt/airflow/notebooks/', '/home/jovyan/work/notebooks/', '/tmp/notebooks/']\n",
    "for location in locations:\n",
    "    result = subprocess.run(['kubectl', 'exec', '-n', 'airflow', '-l', 'component=scheduler', '--', 'find', location, '-name', '*.ipynb', '2>/dev/null'], \n",
    "                           capture_output=True, text=True)\n",
    "    if result.returncode == 0 and result.stdout.strip():\n",
    "        print(f\"Found notebooks in {location}:\")\n",
    "        print(result.stdout)\n",
    "    else:\n",
    "        print(f\"No notebooks found in {location}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679b61ea",
   "metadata": {},
   "source": [
    "## 5. Validate Kubernetes Resources\n",
    "\n",
    "Let's check the overall health of our Kubernetes deployment and ensure all services are properly configured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14abe24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check all pods status in airflow namespace\n",
    "print(\"Checking all Airflow pods status...\")\n",
    "result = subprocess.run(['kubectl', 'get', 'pods', '-n', 'airflow', '-o', 'wide'], capture_output=True, text=True)\n",
    "print(result.stdout)\n",
    "\n",
    "# Check Jupyter Lab pod status\n",
    "print(\"\\nChecking Jupyter Lab pod status...\")\n",
    "result = subprocess.run(['kubectl', 'get', 'pods', '-n', 'jupyter', '-o', 'wide'], capture_output=True, text=True)\n",
    "print(result.stdout)\n",
    "\n",
    "# Check services and ensure they're accessible\n",
    "print(\"\\nChecking services...\")\n",
    "result = subprocess.run(['kubectl', 'get', 'svc', '-A'], capture_output=True, text=True)\n",
    "print(result.stdout)\n",
    "\n",
    "# Check if there are any recent events that might indicate issues\n",
    "print(\"\\nChecking recent events...\")\n",
    "result = subprocess.run(['kubectl', 'get', 'events', '--sort-by=.metadata.creationTimestamp', '-A', '--tail=10'], capture_output=True, text=True)\n",
    "print(result.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a4db46",
   "metadata": {},
   "source": [
    "## 6. Test Notebook Execution\n",
    "\n",
    "Let's test if we can execute notebooks using Papermill from within the Kubernetes environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd05e260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Papermill execution from Airflow scheduler\n",
    "print(\"Testing Papermill execution capability...\")\n",
    "\n",
    "# First, let's see if we can list Jupyter notebooks from the Airflow scheduler\n",
    "print(\"Checking if notebooks are accessible from Airflow:\")\n",
    "result = subprocess.run(['kubectl', 'exec', '-n', 'airflow', '-l', 'component=scheduler', '--', 'find', '/opt/airflow/', '-name', '*.ipynb'], \n",
    "                       capture_output=True, text=True)\n",
    "if result.stdout:\n",
    "    print(\"Found notebooks:\")\n",
    "    print(result.stdout)\n",
    "else:\n",
    "    print(\"No notebooks found in /opt/airflow/\")\n",
    "\n",
    "# Check if notebooks exist in Jupyter pod\n",
    "print(\"\\nChecking notebooks in Jupyter pod:\")\n",
    "result = subprocess.run(['kubectl', 'exec', '-n', 'jupyter', '-l', 'app=jupyter', '--', 'find', '/home/jovyan/', '-name', '*.ipynb'], \n",
    "                       capture_output=True, text=True)\n",
    "if result.stdout:\n",
    "    print(\"Found notebooks in Jupyter:\")\n",
    "    print(result.stdout)\n",
    "else:\n",
    "    print(\"No notebooks found in Jupyter\")\n",
    "\n",
    "# Test basic Papermill functionality\n",
    "print(\"\\nTesting basic Papermill import and version:\")\n",
    "result = subprocess.run(['kubectl', 'exec', '-n', 'airflow', '-l', 'component=scheduler', '--', 'python', '-c', \n",
    "                        'import papermill; import sys; print(f\"Papermill {papermill.__version__} on Python {sys.version}\")'], \n",
    "                       capture_output=True, text=True)\n",
    "print(result.stdout if result.returncode == 0 else f\"Error: {result.stderr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffabe42b",
   "metadata": {},
   "source": [
    "## 7. File Permissions and Access Troubleshooting\n",
    "\n",
    "Let's check file permissions and ensure proper access to DAGs and notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e880709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check file permissions for DAG files\n",
    "print(\"Checking DAG file permissions...\")\n",
    "result = subprocess.run(['kubectl', 'exec', '-n', 'airflow', '-l', 'component=scheduler', '--', 'ls', '-la', '/opt/airflow/dags/'], \n",
    "                       capture_output=True, text=True)\n",
    "print(result.stdout if result.returncode == 0 else f\"Error: {result.stderr}\")\n",
    "\n",
    "# Check who is the current user in the Airflow scheduler\n",
    "print(\"\\nChecking current user in Airflow scheduler:\")\n",
    "result = subprocess.run(['kubectl', 'exec', '-n', 'airflow', '-l', 'component=scheduler', '--', 'whoami'], \n",
    "                       capture_output=True, text=True)\n",
    "print(f\"Current user: {result.stdout.strip()}\")\n",
    "\n",
    "# Check if we can read the DAG files\n",
    "print(\"\\nTesting DAG file readability:\")\n",
    "result = subprocess.run(['kubectl', 'exec', '-n', 'airflow', '-l', 'component=scheduler', '--', 'python', '-c', \n",
    "                        'import os; files = [f for f in os.listdir(\"/opt/airflow/dags/\") if f.endswith(\".py\")]; print(f\"Python files: {files}\")'], \n",
    "                       capture_output=True, text=True)\n",
    "print(result.stdout if result.returncode == 0 else f\"Error: {result.stderr}\")\n",
    "\n",
    "# Check Airflow configuration for DAG folder\n",
    "print(\"\\nChecking Airflow DAG folder configuration:\")\n",
    "result = subprocess.run(['kubectl', 'exec', '-n', 'airflow', '-l', 'component=scheduler', '--', 'python', '-c', \n",
    "                        'from airflow.configuration import conf; print(f\"DAGs folder: {conf.get(\\\\\"core\\\\\", \\\\\"dags_folder\\\\\")}\")'], \n",
    "                       capture_output=True, text=True)\n",
    "print(result.stdout if result.returncode == 0 else f\"Error: {result.stderr}\")\n",
    "\n",
    "# Force DAG folder scan\n",
    "print(\"\\nForcing DAG folder scan...\")\n",
    "result = subprocess.run(['kubectl', 'exec', '-n', 'airflow', '-l', 'component=scheduler', '--', 'airflow', 'dags', 'list'], \n",
    "                       capture_output=True, text=True)\n",
    "print(\"DAG scan result:\")\n",
    "print(result.stdout if result.returncode == 0 else f\"Error: {result.stderr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6cb100",
   "metadata": {},
   "source": [
    "## 8. Next Steps and Recommendations\n",
    "\n",
    "Based on the results above, here are the recommended next steps:\n",
    "\n",
    "### If DAGs are still not visible:\n",
    "1. **Restart Airflow scheduler**: Sometimes a restart helps pick up new DAG files\n",
    "2. **Check DAG syntax**: Ensure there are no Python syntax errors in the DAG files\n",
    "3. **Verify imports**: Make sure all required Python packages are installed in the Airflow environment\n",
    "4. **Check Airflow configuration**: Verify DAG folder configuration and scanning intervals\n",
    "\n",
    "### If Notebooks are not accessible:\n",
    "1. **Access Jupyter Lab**: Forward the Jupyter service port and access the web interface\n",
    "2. **Copy notebooks to correct location**: Ensure notebooks are in the expected Jupyter working directory\n",
    "3. **Install required packages**: Install Papermill and other dependencies in the Jupyter environment\n",
    "\n",
    "### Verification Commands:\n",
    "- Check Airflow UI: `kubectl port-forward -n airflow svc/airflow-webserver 8080:8080`\n",
    "- Check Jupyter Lab: `kubectl port-forward -n jupyter svc/jupyter 8888:8888`\n",
    "- Restart Airflow scheduler: `kubectl delete pod -n airflow -l component=scheduler`\n",
    "\n",
    "Run the cells above to get specific diagnostic information, then follow the appropriate recommendations based on your results."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
