{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac7a4b84",
   "metadata": {},
   "source": [
    "# Data Ingestion Notebook\n",
    "\n",
    "This notebook demonstrates data ingestion from various sources into S3 (LocalStack).\n",
    "\n",
    "## Parameters\n",
    "- `execution_date`: Date of pipeline execution\n",
    "- `s3_bucket`: Target S3 bucket for data storage\n",
    "- `aws_endpoint_url`: AWS endpoint URL (LocalStack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7166d41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters - these will be injected by Papermill\n",
    "execution_date = '2024-01-01'\n",
    "s3_bucket = 'data-lake'\n",
    "aws_endpoint_url = 'http://localhost:4566'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3807c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import boto3\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# Configure AWS credentials for LocalStack\n",
    "os.environ['AWS_ACCESS_KEY_ID'] = 'test'\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = 'test'\n",
    "os.environ['AWS_DEFAULT_REGION'] = 'us-east-1'\n",
    "\n",
    "print(f\"Execution Date: {execution_date}\")\n",
    "print(f\"S3 Bucket: {s3_bucket}\")\n",
    "print(f\"AWS Endpoint: {aws_endpoint_url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b5878b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize S3 client\n",
    "s3_client = boto3.client(\n",
    "    's3',\n",
    "    endpoint_url=aws_endpoint_url,\n",
    "    aws_access_key_id='test',\n",
    "    aws_secret_access_key='test',\n",
    "    region_name='us-east-1'\n",
    ")\n",
    "\n",
    "# Check if bucket exists, create if not\n",
    "try:\n",
    "    s3_client.head_bucket(Bucket=s3_bucket)\n",
    "    print(f\"Bucket '{s3_bucket}' exists\")\n",
    "except:\n",
    "    s3_client.create_bucket(Bucket=s3_bucket)\n",
    "    print(f\"Created bucket '{s3_bucket}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2194a5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample data\n",
    "np.random.seed(42)\n",
    "n_records = 10000\n",
    "\n",
    "sample_data = pd.DataFrame({\n",
    "    'customer_id': range(1, n_records + 1),\n",
    "    'transaction_date': pd.date_range(\n",
    "        start=execution_date, \n",
    "        periods=n_records, \n",
    "        freq='min'\n",
    "    ),\n",
    "    'amount': np.random.normal(100, 25, n_records),\n",
    "    'category': np.random.choice(\n",
    "        ['Electronics', 'Clothing', 'Food', 'Books', 'Other'], \n",
    "        n_records\n",
    "    ),\n",
    "    'payment_method': np.random.choice(\n",
    "        ['Credit Card', 'Debit Card', 'Cash', 'PayPal'], \n",
    "        n_records\n",
    "    )\n",
    "})\n",
    "\n",
    "# Add some missing values for data quality testing\n",
    "sample_data.loc[np.random.choice(sample_data.index, 100), 'amount'] = np.nan\n",
    "sample_data.loc[np.random.choice(sample_data.index, 50), 'category'] = None\n",
    "\n",
    "print(f\"Generated {len(sample_data)} records\")\n",
    "print(sample_data.head())\n",
    "print(\"\\nData Info:\")\n",
    "print(sample_data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7958fcb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data to CSV\n",
    "csv_filename = f\"raw_data_{execution_date.replace('-', '_')}.csv\"\n",
    "csv_path = f\"/tmp/{csv_filename}\"\n",
    "\n",
    "sample_data.to_csv(csv_path, index=False)\n",
    "print(f\"Saved data to {csv_path}\")\n",
    "\n",
    "# Upload to S3\n",
    "s3_key = f\"raw-data/{execution_date}/{csv_filename}\"\n",
    "s3_client.upload_file(csv_path, s3_bucket, s3_key)\n",
    "\n",
    "print(f\"Uploaded to S3: s3://{s3_bucket}/{s3_key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a26e970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify upload\n",
    "try:\n",
    "    response = s3_client.head_object(Bucket=s3_bucket, Key=s3_key)\n",
    "    print(f\"✅ File successfully uploaded to S3\")\n",
    "    print(f\"File size: {response['ContentLength']} bytes\")\n",
    "    print(f\"Last modified: {response['LastModified']}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error verifying upload: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da42a83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate metadata\n",
    "metadata = {\n",
    "    'execution_date': execution_date,\n",
    "    'total_records': len(sample_data),\n",
    "    'file_path': f\"s3://{s3_bucket}/{s3_key}\",\n",
    "    'missing_values': {\n",
    "        'amount': sample_data['amount'].isna().sum(),\n",
    "        'category': sample_data['category'].isna().sum()\n",
    "    },\n",
    "    'categories': sample_data['category'].value_counts().to_dict(),\n",
    "    'timestamp': datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "print(\"Data Ingestion Metadata:\")\n",
    "for key, value in metadata.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35aaa3bf",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Data ingestion completed successfully!\n",
    "\n",
    "The following steps were performed:\n",
    "1. Generated sample transaction data\n",
    "2. Added realistic missing values for testing\n",
    "3. Uploaded data to S3 (LocalStack)\n",
    "4. Generated metadata for downstream processing\n",
    "\n",
    "The data is now ready for processing by the Spark job."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
